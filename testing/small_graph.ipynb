{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 8347] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 2229] },\n",
      "  (drug, targets, target)={ edge_index=[2, 338] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 338] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 41415] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Load encoded data from CSV files\n",
    "encoded_drugbank_id_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_drugbank_id.csv')\n",
    "encoded_name_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_name.csv')\n",
    "encoded_state_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_state.csv')\n",
    "encoded_groups_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_groups.csv')\n",
    "encoded_categories_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_categories.csv')\n",
    "encoded_atc_codes_df = pd.read_csv('Heterogeneous KG\\encoders_small\\encoded_atc_codes.csv')\n",
    "encoded_targets_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_targets.csv')\n",
    "encoded_interactions_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_interactions.csv')\n",
    "encoded_molecular_formula_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_molecular_formula.csv')\n",
    "encoded_doping_df = pd.read_csv('Heterogeneous KG\\encoders_small/encoded_doping.csv')\n",
    "\n",
    "# Convert DataFrames to tensors\n",
    "encoded_drugbank_id_tensor_1 = torch.tensor(encoded_drugbank_id_df.values, dtype=torch.float32)\n",
    "encoded_name_tensor_1 = torch.tensor(encoded_name_df.values, dtype=torch.float32)\n",
    "encoded_state_tensor_1 = torch.tensor(encoded_state_df.values, dtype=torch.float32)\n",
    "encoded_groups_tensor_1 = torch.tensor(encoded_groups_df.values, dtype=torch.float32)\n",
    "encoded_categories_tensor_1 = torch.tensor(encoded_categories_df.values, dtype=torch.float32)\n",
    "encoded_atc_codes_tensor_1 = torch.tensor(encoded_atc_codes_df.values, dtype=torch.float32)\n",
    "encoded_targets_tensor_1 = torch.tensor(encoded_targets_df.values, dtype=torch.float32)\n",
    "encoded_interactions_tensor_1 = torch.tensor(encoded_interactions_df.values, dtype=torch.float32)\n",
    "encoded_molecular_formula_tensor_1 = torch.tensor(encoded_molecular_formula_df.values, dtype=torch.float32)\n",
    "encoded_doping_tensor_1 = torch.tensor(encoded_doping_df.values, dtype=torch.float32)\n",
    "\n",
    "# Initialize HeteroData\n",
    "data_small = HeteroData()\n",
    "\n",
    "# Add Drug node features\n",
    "data_small['drug'].x = torch.cat([\n",
    "    encoded_drugbank_id_tensor_1,\n",
    "    encoded_name_tensor_1,\n",
    "    encoded_state_tensor_1,\n",
    "    encoded_groups_tensor_1,\n",
    "    encoded_molecular_formula_tensor_1\n",
    "], dim=1)\n",
    "\n",
    "# Add Drug Category nodes (one-hot encoding)\n",
    "data_small['drug_category'].x = torch.eye(len(encoded_categories_df.columns), dtype=torch.float32)\n",
    "\n",
    "# Add ATC Code nodes (one-hot encoding)\n",
    "data_small['atc_code'].x = torch.eye(len(encoded_atc_codes_df.columns), dtype=torch.float32)\n",
    "\n",
    "# Add Target nodes (one-hot encoding)\n",
    "data_small['target'].x = torch.eye(len(encoded_targets_df.columns), dtype=torch.float32)\n",
    "\n",
    "# Add Doping nodes (one-hot encoding)\n",
    "data_small['doping'].x = torch.eye(len(encoded_doping_df['Doping'].unique()), dtype=torch.float32)\n",
    "\n",
    "# Create edge lists for drug-to-category relationships\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "for drug_idx, row in encoded_categories_df.iterrows():\n",
    "    for category_idx in range(len(row)):\n",
    "        if row[category_idx] == 1:\n",
    "            source_nodes.append(drug_idx)\n",
    "            target_nodes.append(category_idx)\n",
    "data_small['drug', 'isInCategory', 'drug_category'].edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "# Create edge lists for drug-to-ATC code relationships\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "for drug_idx, row in encoded_atc_codes_df.iterrows():\n",
    "    for atc_code_idx in range(len(row)):\n",
    "        if row[atc_code_idx] == 1:\n",
    "            source_nodes.append(drug_idx)\n",
    "            target_nodes.append(atc_code_idx)\n",
    "data_small['drug', 'isClassifiedAs', 'atc_code'].edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "# Create edge lists for drug-to-target relationships\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "for drug_idx, row in encoded_targets_df.iterrows():\n",
    "    for target_idx in range(len(row)):\n",
    "        if row[target_idx] == 1:\n",
    "            source_nodes.append(drug_idx)\n",
    "            target_nodes.append(target_idx)\n",
    "data_small['drug', 'targets', 'target'].edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "# Create edge lists for drug-to-doping relationships\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "for drug_idx, doping in enumerate(encoded_doping_df['Doping']):\n",
    "    source_nodes.append(drug_idx)\n",
    "    target_nodes.append(doping)\n",
    "data_small['drug', 'isDoping', 'doping'].edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "# Create edge lists for drug-to-drug interactions\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "for drug_idx, row in encoded_interactions_df.iterrows():\n",
    "    for target_idx in range(len(row)):\n",
    "        if row[target_idx] == 1:\n",
    "            source_nodes.append(drug_idx)\n",
    "            target_nodes.append(target_idx)\n",
    "data_small['drug', 'interactsWith', 'drug'].edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "print(data_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6010] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1605] },\n",
      "  (drug, targets, target)={ edge_index=[2, 243] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 243] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46112] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 667] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 178] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5123] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1670] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 446] },\n",
      "  (drug, targets, target)={ edge_index=[2, 68] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 68] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12809] }\n",
      ")\n",
      "Fold 2\n",
      "Train Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6010] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1605] },\n",
      "  (drug, targets, target)={ edge_index=[2, 243] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 243] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46112] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 667] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 178] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5123] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1670] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 446] },\n",
      "  (drug, targets, target)={ edge_index=[2, 68] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 68] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12809] }\n",
      ")\n",
      "Fold 3\n",
      "Train Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6011] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1605] },\n",
      "  (drug, targets, target)={ edge_index=[2, 243] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 243] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46112] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 667] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 178] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5123] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1669] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 446] },\n",
      "  (drug, targets, target)={ edge_index=[2, 68] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 68] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12809] }\n",
      ")\n",
      "Fold 4\n",
      "Train Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6011] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1605] },\n",
      "  (drug, targets, target)={ edge_index=[2, 244] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 244] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46112] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 667] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 178] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5123] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1669] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 446] },\n",
      "  (drug, targets, target)={ edge_index=[2, 67] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 67] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12809] }\n",
      ")\n",
      "Fold 5\n",
      "Train Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6011] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1606] },\n",
      "  (drug, targets, target)={ edge_index=[2, 244] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 244] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46113] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 667] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 178] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5123] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={ x=[338, 467] },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1669] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 445] },\n",
      "  (drug, targets, target)={ edge_index=[2, 67] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 67] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12808] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assume data is your input HeteroData\n",
    "data = ToUndirected()(data_small)\n",
    "\n",
    "def generate_k_folds(data, edge_type, k=5):\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    num_edges = edge_index.size(1)\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    return kf.split(range(num_edges))\n",
    "\n",
    "def split_edges(data, edge_type, train_indices, test_indices, val_split=0.1):\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    \n",
    "    # Select train and test edges\n",
    "    train_edge_index = edge_index[:, train_indices]\n",
    "    test_edge_index = edge_index[:, test_indices]\n",
    "    \n",
    "    # Split train indices further into train and validation\n",
    "    val_size = int(len(train_indices) * val_split)\n",
    "    train_indices, val_indices = train_indices[:-val_size], train_indices[-val_size:]\n",
    "    \n",
    "    val_edge_index = edge_index[:, val_indices]\n",
    "    train_edge_index = edge_index[:, train_indices]\n",
    "    \n",
    "    return train_edge_index, val_edge_index, test_edge_index\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "k = 5  # or 10 for 10-fold cross-validation\n",
    "edge_types = [\n",
    "    ('drug', 'isInCategory', 'drug_category'),\n",
    "    ('drug', 'isClassifiedAs', 'atc_code'),\n",
    "    ('drug', 'targets', 'target'),\n",
    "    ('drug', 'isDoping', 'doping'),\n",
    "    ('drug', 'interactsWith', 'drug')\n",
    "]\n",
    "kf_splits = {edge_type: list(generate_k_folds(data, edge_type, k)) for edge_type in edge_types}\n",
    "\n",
    "for fold in range(k):\n",
    "    train_data = HeteroData()\n",
    "    val_data = HeteroData()\n",
    "    test_data = HeteroData()\n",
    "    \n",
    "    for edge_type in edge_types:\n",
    "        train_indices, test_indices = kf_splits[edge_type][fold]\n",
    "        train_edge_index, val_edge_index, test_edge_index = split_edges(data, edge_type, train_indices, test_indices)\n",
    "        \n",
    "        train_data[edge_type].edge_index = train_edge_index\n",
    "        val_data[edge_type].edge_index = val_edge_index\n",
    "        test_data[edge_type].edge_index = test_edge_index\n",
    "        \n",
    "        if 'x' in data[edge_type[0]]:\n",
    "            train_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "            val_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "            test_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "            \n",
    "        if 'x' in data[edge_type[2]]:\n",
    "            train_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "            val_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "            test_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    print(\"Train Data:\", train_data)\n",
    "    print(\"Validation Data:\", val_data)\n",
    "    print(\"Test Data:\", test_data)\n",
    "    \n",
    "    # Here, you can train your model using train_data, validate using val_data, and test using test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40) tensor(298) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "# Load encoded data from CSV files and convert to tensors (omitted for brevity)\n",
    "# Initialize HeteroData and add nodes and edges (omitted for brevity)\n",
    "\n",
    "# Add doping labels to the drug nodes\n",
    "data['drug'].y = torch.tensor(encoded_doping_df['Doping'].values, dtype=torch.long)\n",
    "\n",
    "# Perform a node-level random split\n",
    "transform = RandomNodeSplit(split='random', num_splits=1)\n",
    "data = transform(data)\n",
    "\n",
    "# Verify masks\n",
    "print(data['drug'].train_mask.sum(), data['drug'].val_mask.sum(), data['drug'].test_mask.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a tensor 'y' with labels for each node\n",
    "num_nodes = data['drug'].x.size(0)\n",
    "\n",
    "# Create train/val/test masks\n",
    "train_mask, test_mask = train_test_split(range(num_nodes), test_size=0.2, random_state=42)\n",
    "train_mask, val_mask = train_test_split(train_mask, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize masks\n",
    "data['drug'].train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data['drug'].val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data['drug'].test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Assign masks\n",
    "data['drug'].train_mask[train_mask] = True\n",
    "data['drug'].val_mask[val_mask] = True\n",
    "data['drug'].test_mask[test_mask] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GATConv, Linear, SAGEConv, GraphConv\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): GraphConv((-1, -1), hidden_channels),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): GraphConv((-1, -1), hidden_channels),\n",
    "            ('drug', 'targets', 'target'): GraphConv((-1, -1), hidden_channels),\n",
    "            ('drug', 'isDoping', 'doping'): GraphConv((-1, -1), hidden_channels),\n",
    "            ('drug', 'interactsWith', 'drug'): GraphConv((-1, -1), hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        # Adjust the linear layer to match the output dimension of GATConv\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Instantiate the model\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GraphConv, Linear\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'targets', 'target'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'isDoping', 'doping'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'interactsWith', 'drug'): GraphConv(-1, hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Instantiate the model\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "\n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "\n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 002, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 003, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 004, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 005, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 006, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 007, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 008, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 009, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 010, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 011, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 012, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 013, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 014, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 015, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 016, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 017, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 018, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 019, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 020, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 021, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 022, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 023, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 024, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 025, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 026, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 027, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 028, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 029, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 030, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 031, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 032, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 033, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 034, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 035, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 036, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 037, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 038, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 039, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 040, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 041, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 042, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 043, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 044, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 045, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 046, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 047, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 048, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 049, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 050, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 051, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 052, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 053, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 054, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 055, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 056, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 057, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 058, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 059, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 060, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 061, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 062, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 063, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 064, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 065, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 066, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 067, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 068, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 069, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 070, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 071, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 072, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 073, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 074, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 075, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 076, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 077, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 078, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 079, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 080, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 081, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 082, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 083, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 084, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 085, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 086, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 087, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 088, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 089, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 090, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 091, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 092, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 093, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 094, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 095, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 096, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 097, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 098, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 099, Loss: 123.5039, Train Acc: 0.3868\n",
      "Epoch: 100, Loss: 123.5039, Train Acc: 0.3868\n",
      "Average Train Precision: 0.1934, Average Train Recall: 0.5000, Average Train F1: 0.2789\n",
      "Average Val Precision: 0.1852, Average Val Recall: 0.5000, Average Val F1: 0.2703\n",
      "Test Acc: 0.3676, Test Precision: 0.1838, Test Recall: 0.5000, Test F1: 0.2688\n"
     ]
    }
   ],
   "source": [
    "# To accumulate metrics across epochs\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(data['drug'].val_mask)\n",
    "    test_acc, test_precision, test_recall, test_f1 = test(data['drug'].test_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 4.7782, Train Acc: 0.3868\n",
      "Epoch: 002, Loss: 1928.8292, Train Acc: 0.3868\n",
      "Epoch: 003, Loss: 973.8771, Train Acc: 0.3868\n",
      "Epoch: 004, Loss: 32.6837, Train Acc: 0.6132\n",
      "Epoch: 005, Loss: 317.9927, Train Acc: 0.6132\n",
      "Epoch: 006, Loss: 434.1130, Train Acc: 0.6132\n",
      "Epoch: 007, Loss: 449.3937, Train Acc: 0.6132\n",
      "Epoch: 008, Loss: 404.1190, Train Acc: 0.6132\n",
      "Epoch: 009, Loss: 319.5454, Train Acc: 0.6132\n",
      "Epoch: 010, Loss: 210.3478, Train Acc: 0.6173\n",
      "Epoch: 011, Loss: 84.9438, Train Acc: 0.3868\n",
      "Epoch: 012, Loss: 87.9344, Train Acc: 0.3868\n",
      "Epoch: 013, Loss: 155.4108, Train Acc: 0.3868\n",
      "Epoch: 014, Loss: 135.0769, Train Acc: 0.3868\n",
      "Epoch: 015, Loss: 67.4123, Train Acc: 0.6132\n",
      "Epoch: 016, Loss: 19.9105, Train Acc: 0.6173\n",
      "Epoch: 017, Loss: 53.8865, Train Acc: 0.6173\n",
      "Epoch: 018, Loss: 67.6360, Train Acc: 0.6173\n",
      "Epoch: 019, Loss: 65.1847, Train Acc: 0.6173\n",
      "Epoch: 020, Loss: 49.8084, Train Acc: 0.6132\n",
      "Epoch: 021, Loss: 24.3386, Train Acc: 0.3868\n",
      "Epoch: 022, Loss: 13.9826, Train Acc: 0.3868\n",
      "Epoch: 023, Loss: 29.7222, Train Acc: 0.3868\n",
      "Epoch: 024, Loss: 12.5343, Train Acc: 0.6132\n",
      "Epoch: 025, Loss: 16.8394, Train Acc: 0.6132\n",
      "Epoch: 026, Loss: 29.3549, Train Acc: 0.6132\n",
      "Epoch: 027, Loss: 31.3730, Train Acc: 0.6091\n",
      "Epoch: 028, Loss: 24.4996, Train Acc: 0.6132\n",
      "Epoch: 029, Loss: 10.2890, Train Acc: 0.3868\n",
      "Epoch: 030, Loss: 16.0955, Train Acc: 0.3868\n",
      "Epoch: 031, Loss: 23.3345, Train Acc: 0.3868\n",
      "Epoch: 032, Loss: 7.1848, Train Acc: 0.6132\n",
      "Epoch: 033, Loss: 15.4504, Train Acc: 0.6132\n",
      "Epoch: 034, Loss: 26.5029, Train Acc: 0.6132\n",
      "Epoch: 035, Loss: 29.3998, Train Acc: 0.6132\n",
      "Epoch: 036, Loss: 25.1976, Train Acc: 0.6132\n",
      "Epoch: 037, Loss: 14.9898, Train Acc: 0.6049\n",
      "Epoch: 038, Loss: 1.4027, Train Acc: 0.3868\n",
      "Epoch: 039, Loss: 23.1642, Train Acc: 0.3868\n",
      "Epoch: 040, Loss: 25.8311, Train Acc: 0.3868\n",
      "Epoch: 041, Loss: 10.3298, Train Acc: 0.6173\n",
      "Epoch: 042, Loss: 10.8475, Train Acc: 0.6132\n",
      "Epoch: 043, Loss: 20.8535, Train Acc: 0.6132\n",
      "Epoch: 044, Loss: 24.2241, Train Acc: 0.6132\n",
      "Epoch: 045, Loss: 21.8002, Train Acc: 0.6132\n",
      "Epoch: 046, Loss: 14.4117, Train Acc: 0.6173\n",
      "Epoch: 047, Loss: 3.3043, Train Acc: 0.3868\n",
      "Epoch: 048, Loss: 18.7899, Train Acc: 0.3868\n",
      "Epoch: 049, Loss: 26.0632, Train Acc: 0.3868\n",
      "Epoch: 050, Loss: 18.0564, Train Acc: 0.5885\n",
      "Epoch: 051, Loss: 1.7436, Train Acc: 0.6214\n",
      "Epoch: 052, Loss: 8.7093, Train Acc: 0.6173\n",
      "Epoch: 053, Loss: 11.3444, Train Acc: 0.6173\n",
      "Epoch: 054, Loss: 9.4804, Train Acc: 0.6379\n",
      "Epoch: 055, Loss: 4.0020, Train Acc: 0.3868\n",
      "Epoch: 056, Loss: 7.5140, Train Acc: 0.3868\n",
      "Epoch: 057, Loss: 8.7359, Train Acc: 0.5802\n",
      "Epoch: 058, Loss: 1.4808, Train Acc: 0.6337\n",
      "Epoch: 059, Loss: 4.3461, Train Acc: 0.6337\n",
      "Epoch: 060, Loss: 4.1346, Train Acc: 0.5926\n",
      "Epoch: 061, Loss: 1.3714, Train Acc: 0.3868\n",
      "Epoch: 062, Loss: 6.4386, Train Acc: 0.4156\n",
      "Epoch: 063, Loss: 2.7358, Train Acc: 0.6420\n",
      "Epoch: 064, Loss: 5.3302, Train Acc: 0.6173\n",
      "Epoch: 065, Loss: 8.5227, Train Acc: 0.6173\n",
      "Epoch: 066, Loss: 8.0644, Train Acc: 0.6337\n",
      "Epoch: 067, Loss: 4.4711, Train Acc: 0.4115\n",
      "Epoch: 068, Loss: 2.6793, Train Acc: 0.3909\n",
      "Epoch: 069, Loss: 3.5692, Train Acc: 0.6173\n",
      "Epoch: 070, Loss: 2.7605, Train Acc: 0.6337\n",
      "Epoch: 071, Loss: 4.3486, Train Acc: 0.6337\n",
      "Epoch: 072, Loss: 3.1061, Train Acc: 0.4938\n",
      "Epoch: 073, Loss: 1.4777, Train Acc: 0.4033\n",
      "Epoch: 074, Loss: 2.6644, Train Acc: 0.6255\n",
      "Epoch: 075, Loss: 2.3532, Train Acc: 0.6379\n",
      "Epoch: 076, Loss: 3.3033, Train Acc: 0.6255\n",
      "Epoch: 077, Loss: 1.8396, Train Acc: 0.3951\n",
      "Epoch: 078, Loss: 3.2958, Train Acc: 0.5638\n",
      "Epoch: 079, Loss: 1.1995, Train Acc: 0.6173\n",
      "Epoch: 080, Loss: 2.4819, Train Acc: 0.6173\n",
      "Epoch: 081, Loss: 2.5801, Train Acc: 0.5967\n",
      "Epoch: 082, Loss: 1.0637, Train Acc: 0.3868\n",
      "Epoch: 083, Loss: 3.5202, Train Acc: 0.5761\n",
      "Epoch: 084, Loss: 1.1297, Train Acc: 0.6255\n",
      "Epoch: 085, Loss: 2.0901, Train Acc: 0.5926\n",
      "Epoch: 086, Loss: 1.3593, Train Acc: 0.4033\n",
      "Epoch: 087, Loss: 2.3846, Train Acc: 0.5885\n",
      "Epoch: 088, Loss: 1.0880, Train Acc: 0.6214\n",
      "Epoch: 089, Loss: 1.5957, Train Acc: 0.5885\n",
      "Epoch: 090, Loss: 1.0035, Train Acc: 0.4321\n",
      "Epoch: 091, Loss: 1.8471, Train Acc: 0.6255\n",
      "Epoch: 092, Loss: 1.4785, Train Acc: 0.6255\n",
      "Epoch: 093, Loss: 1.6908, Train Acc: 0.5926\n",
      "Epoch: 094, Loss: 1.0255, Train Acc: 0.4774\n",
      "Epoch: 095, Loss: 1.5518, Train Acc: 0.6255\n",
      "Epoch: 096, Loss: 1.7242, Train Acc: 0.6296\n",
      "Epoch: 097, Loss: 1.8593, Train Acc: 0.5926\n",
      "Epoch: 098, Loss: 0.9925, Train Acc: 0.4362\n",
      "Epoch: 099, Loss: 1.7752, Train Acc: 0.6296\n",
      "Epoch: 100, Loss: 1.7917, Train Acc: 0.6379\n",
      "Epoch: 101, Loss: 2.1242, Train Acc: 0.5926\n",
      "Epoch: 102, Loss: 0.9097, Train Acc: 0.3868\n",
      "Epoch: 103, Loss: 2.8266, Train Acc: 0.6049\n",
      "Epoch: 104, Loss: 1.1853, Train Acc: 0.6379\n",
      "Epoch: 105, Loss: 1.8088, Train Acc: 0.6132\n",
      "Epoch: 106, Loss: 0.9099, Train Acc: 0.3951\n",
      "Epoch: 107, Loss: 2.4266, Train Acc: 0.6255\n",
      "Epoch: 108, Loss: 1.2340, Train Acc: 0.6379\n",
      "Epoch: 109, Loss: 1.6855, Train Acc: 0.6173\n",
      "Epoch: 110, Loss: 0.8583, Train Acc: 0.4033\n",
      "Epoch: 111, Loss: 1.9452, Train Acc: 0.6296\n",
      "Epoch: 112, Loss: 1.5498, Train Acc: 0.6337\n",
      "Epoch: 113, Loss: 1.8854, Train Acc: 0.6132\n",
      "Epoch: 114, Loss: 0.8379, Train Acc: 0.3951\n",
      "Epoch: 115, Loss: 2.2714, Train Acc: 0.6337\n",
      "Epoch: 116, Loss: 1.4275, Train Acc: 0.6296\n",
      "Epoch: 117, Loss: 1.8798, Train Acc: 0.6008\n",
      "Epoch: 118, Loss: 0.8207, Train Acc: 0.3868\n",
      "Epoch: 119, Loss: 2.4714, Train Acc: 0.6379\n",
      "Epoch: 120, Loss: 1.1956, Train Acc: 0.6379\n",
      "Epoch: 121, Loss: 1.6796, Train Acc: 0.6049\n",
      "Epoch: 122, Loss: 0.7998, Train Acc: 0.3992\n",
      "Epoch: 123, Loss: 2.0531, Train Acc: 0.6337\n",
      "Epoch: 124, Loss: 1.3837, Train Acc: 0.6337\n",
      "Epoch: 125, Loss: 1.7428, Train Acc: 0.6132\n",
      "Epoch: 126, Loss: 0.7879, Train Acc: 0.3992\n",
      "Epoch: 127, Loss: 2.0199, Train Acc: 0.6337\n",
      "Epoch: 128, Loss: 1.4245, Train Acc: 0.6420\n",
      "Epoch: 129, Loss: 1.8037, Train Acc: 0.6132\n",
      "Epoch: 130, Loss: 0.7699, Train Acc: 0.3868\n",
      "Epoch: 131, Loss: 2.2680, Train Acc: 0.6337\n",
      "Epoch: 132, Loss: 1.2089, Train Acc: 0.6420\n",
      "Epoch: 133, Loss: 1.6427, Train Acc: 0.6049\n",
      "Epoch: 134, Loss: 0.7571, Train Acc: 0.3951\n",
      "Epoch: 135, Loss: 1.9786, Train Acc: 0.6337\n",
      "Epoch: 136, Loss: 1.3021, Train Acc: 0.6420\n",
      "Epoch: 137, Loss: 1.6490, Train Acc: 0.6173\n",
      "Epoch: 138, Loss: 0.7497, Train Acc: 0.3992\n",
      "Epoch: 139, Loss: 1.8798, Train Acc: 0.6379\n",
      "Epoch: 140, Loss: 1.3438, Train Acc: 0.6420\n",
      "Epoch: 141, Loss: 1.6821, Train Acc: 0.6049\n",
      "Epoch: 142, Loss: 0.7357, Train Acc: 0.3909\n",
      "Epoch: 143, Loss: 2.0070, Train Acc: 0.6337\n",
      "Epoch: 144, Loss: 1.2183, Train Acc: 0.6420\n",
      "Epoch: 145, Loss: 1.5908, Train Acc: 0.6049\n",
      "Epoch: 146, Loss: 0.7254, Train Acc: 0.3909\n",
      "Epoch: 147, Loss: 1.8766, Train Acc: 0.6379\n",
      "Epoch: 148, Loss: 1.2161, Train Acc: 0.6420\n",
      "Epoch: 149, Loss: 1.5473, Train Acc: 0.6132\n",
      "Epoch: 150, Loss: 0.7198, Train Acc: 0.3992\n",
      "Epoch: 151, Loss: 1.7419, Train Acc: 0.6379\n",
      "Epoch: 152, Loss: 1.2455, Train Acc: 0.6420\n",
      "Epoch: 153, Loss: 1.5451, Train Acc: 0.6132\n",
      "Epoch: 154, Loss: 0.7122, Train Acc: 0.3992\n",
      "Epoch: 155, Loss: 1.7344, Train Acc: 0.6379\n",
      "Epoch: 156, Loss: 1.2058, Train Acc: 0.6420\n",
      "Epoch: 157, Loss: 1.5105, Train Acc: 0.6091\n",
      "Epoch: 158, Loss: 0.7027, Train Acc: 0.3992\n",
      "Epoch: 159, Loss: 1.7102, Train Acc: 0.6379\n",
      "Epoch: 160, Loss: 1.1558, Train Acc: 0.6420\n",
      "Epoch: 161, Loss: 1.4545, Train Acc: 0.6132\n",
      "Epoch: 162, Loss: 0.6965, Train Acc: 0.3992\n",
      "Epoch: 163, Loss: 1.6140, Train Acc: 0.6379\n",
      "Epoch: 164, Loss: 1.1440, Train Acc: 0.6420\n",
      "Epoch: 165, Loss: 1.4142, Train Acc: 0.6091\n",
      "Epoch: 166, Loss: 0.6934, Train Acc: 0.3992\n",
      "Epoch: 167, Loss: 1.5207, Train Acc: 0.6379\n",
      "Epoch: 168, Loss: 1.1390, Train Acc: 0.6420\n",
      "Epoch: 169, Loss: 1.3835, Train Acc: 0.6132\n",
      "Epoch: 170, Loss: 0.6900, Train Acc: 0.3992\n",
      "Epoch: 171, Loss: 1.4574, Train Acc: 0.6379\n",
      "Epoch: 172, Loss: 1.1202, Train Acc: 0.6420\n",
      "Epoch: 173, Loss: 1.3488, Train Acc: 0.6091\n",
      "Epoch: 174, Loss: 0.6859, Train Acc: 0.3992\n",
      "Epoch: 175, Loss: 1.4018, Train Acc: 0.6379\n",
      "Epoch: 176, Loss: 1.0941, Train Acc: 0.6420\n",
      "Epoch: 177, Loss: 1.3075, Train Acc: 0.6049\n",
      "Epoch: 178, Loss: 0.6829, Train Acc: 0.3992\n",
      "Epoch: 179, Loss: 1.3341, Train Acc: 0.6379\n",
      "Epoch: 180, Loss: 1.0707, Train Acc: 0.6420\n",
      "Epoch: 181, Loss: 1.2626, Train Acc: 0.5967\n",
      "Epoch: 182, Loss: 0.6829, Train Acc: 0.4156\n",
      "Epoch: 183, Loss: 1.2505, Train Acc: 0.6379\n",
      "Epoch: 184, Loss: 1.0526, Train Acc: 0.6461\n",
      "Epoch: 185, Loss: 1.2145, Train Acc: 0.5967\n",
      "Epoch: 186, Loss: 0.6880, Train Acc: 0.4362\n",
      "Epoch: 187, Loss: 1.1511, Train Acc: 0.6379\n",
      "Epoch: 188, Loss: 1.0371, Train Acc: 0.6502\n",
      "Epoch: 189, Loss: 1.1579, Train Acc: 0.5926\n",
      "Epoch: 190, Loss: 0.7024, Train Acc: 0.4568\n",
      "Epoch: 191, Loss: 1.0274, Train Acc: 0.6379\n",
      "Epoch: 192, Loss: 1.0173, Train Acc: 0.6461\n",
      "Epoch: 193, Loss: 1.0768, Train Acc: 0.5679\n",
      "Epoch: 194, Loss: 0.7411, Train Acc: 0.5267\n",
      "Epoch: 195, Loss: 0.8590, Train Acc: 0.6379\n",
      "Epoch: 196, Loss: 0.9703, Train Acc: 0.6379\n",
      "Epoch: 197, Loss: 0.9257, Train Acc: 0.5267\n",
      "Epoch: 198, Loss: 0.8499, Train Acc: 0.6008\n",
      "Epoch: 199, Loss: 0.6704, Train Acc: 0.6502\n",
      "Epoch: 200, Loss: 0.8520, Train Acc: 0.6173\n",
      "Average Train Precision: 0.5620, Average Train Recall: 0.5378, Average Train F1: 0.4431\n",
      "Average Val Precision: 0.4502, Average Val Recall: 0.5353, Average Val F1: 0.4464\n",
      "Test Acc: 0.6029, Test Precision: 0.5522, Test Recall: 0.5437, Test F1: 0.5409\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GraphConv, Linear\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'targets', 'target'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'isDoping', 'doping'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'interactsWith', 'drug'): GraphConv(-1, hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Instantiate the model\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "\n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "\n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(data)\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(data, data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(data, data['drug'].val_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(data, data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 5.0682, Train Acc: 0.3868\n",
      "Epoch: 002, Loss: 11.5280, Train Acc: 0.3868\n",
      "Epoch: 003, Loss: 6.8058, Train Acc: 0.5761\n",
      "Epoch: 004, Loss: 0.8202, Train Acc: 0.6255\n",
      "Epoch: 005, Loss: 2.8077, Train Acc: 0.6255\n",
      "Epoch: 006, Loss: 2.9713, Train Acc: 0.6255\n",
      "Epoch: 007, Loss: 2.2653, Train Acc: 0.6337\n",
      "Epoch: 008, Loss: 1.0935, Train Acc: 0.4280\n",
      "Epoch: 009, Loss: 0.8134, Train Acc: 0.3827\n",
      "Epoch: 010, Loss: 1.5713, Train Acc: 0.3909\n",
      "Epoch: 011, Loss: 1.5116, Train Acc: 0.4362\n",
      "Epoch: 012, Loss: 0.9085, Train Acc: 0.6420\n",
      "Epoch: 013, Loss: 0.6152, Train Acc: 0.6337\n",
      "Epoch: 014, Loss: 0.8766, Train Acc: 0.6337\n",
      "Epoch: 015, Loss: 1.0654, Train Acc: 0.6337\n",
      "Epoch: 016, Loss: 1.0398, Train Acc: 0.6337\n",
      "Epoch: 017, Loss: 0.8618, Train Acc: 0.6379\n",
      "Epoch: 018, Loss: 0.6623, Train Acc: 0.6667\n",
      "Epoch: 019, Loss: 0.6067, Train Acc: 0.5432\n",
      "Epoch: 020, Loss: 0.7071, Train Acc: 0.4774\n",
      "Epoch: 021, Loss: 0.8043, Train Acc: 0.4815\n",
      "Epoch: 022, Loss: 0.8044, Train Acc: 0.5021\n",
      "Epoch: 023, Loss: 0.7243, Train Acc: 0.6049\n",
      "Epoch: 024, Loss: 0.6348, Train Acc: 0.6708\n",
      "Epoch: 025, Loss: 0.5982, Train Acc: 0.6420\n",
      "Epoch: 026, Loss: 0.6191, Train Acc: 0.6379\n",
      "Epoch: 027, Loss: 0.6596, Train Acc: 0.6337\n",
      "Epoch: 028, Loss: 0.6837, Train Acc: 0.6337\n",
      "Epoch: 029, Loss: 0.6784, Train Acc: 0.6379\n",
      "Epoch: 030, Loss: 0.6508, Train Acc: 0.6420\n",
      "Epoch: 031, Loss: 0.6182, Train Acc: 0.6461\n",
      "Epoch: 032, Loss: 0.5975, Train Acc: 0.6996\n",
      "Epoch: 033, Loss: 0.5961, Train Acc: 0.6749\n",
      "Epoch: 034, Loss: 0.6085, Train Acc: 0.6420\n",
      "Epoch: 035, Loss: 0.6231, Train Acc: 0.6214\n",
      "Epoch: 036, Loss: 0.6299, Train Acc: 0.6255\n",
      "Epoch: 037, Loss: 0.6260, Train Acc: 0.6667\n",
      "Epoch: 038, Loss: 0.6143, Train Acc: 0.6790\n",
      "Epoch: 039, Loss: 0.6015, Train Acc: 0.6872\n",
      "Epoch: 040, Loss: 0.5932, Train Acc: 0.6502\n",
      "Epoch: 041, Loss: 0.5921, Train Acc: 0.6379\n",
      "Epoch: 042, Loss: 0.5972, Train Acc: 0.6461\n",
      "Epoch: 043, Loss: 0.6032, Train Acc: 0.6461\n",
      "Epoch: 044, Loss: 0.6046, Train Acc: 0.6461\n",
      "Epoch: 045, Loss: 0.6019, Train Acc: 0.6461\n",
      "Epoch: 046, Loss: 0.5974, Train Acc: 0.6420\n",
      "Epoch: 047, Loss: 0.5929, Train Acc: 0.6502\n",
      "Epoch: 048, Loss: 0.5898, Train Acc: 0.6996\n",
      "Epoch: 049, Loss: 0.5889, Train Acc: 0.6955\n",
      "Epoch: 050, Loss: 0.5900, Train Acc: 0.6914\n",
      "Epoch: 051, Loss: 0.5914, Train Acc: 0.6996\n",
      "Epoch: 052, Loss: 0.5917, Train Acc: 0.6996\n",
      "Epoch: 053, Loss: 0.5899, Train Acc: 0.6831\n",
      "Epoch: 054, Loss: 0.5864, Train Acc: 0.6914\n",
      "Epoch: 055, Loss: 0.5825, Train Acc: 0.6914\n",
      "Epoch: 056, Loss: 0.5802, Train Acc: 0.6708\n",
      "Epoch: 057, Loss: 0.5808, Train Acc: 0.6543\n",
      "Epoch: 058, Loss: 0.5820, Train Acc: 0.6543\n",
      "Epoch: 059, Loss: 0.5816, Train Acc: 0.6667\n",
      "Epoch: 060, Loss: 0.5789, Train Acc: 0.6914\n",
      "Epoch: 061, Loss: 0.5758, Train Acc: 0.6955\n",
      "Epoch: 062, Loss: 0.5743, Train Acc: 0.6996\n",
      "Epoch: 063, Loss: 0.5749, Train Acc: 0.6914\n",
      "Epoch: 064, Loss: 0.5749, Train Acc: 0.6955\n",
      "Epoch: 065, Loss: 0.5733, Train Acc: 0.6914\n",
      "Epoch: 066, Loss: 0.5707, Train Acc: 0.6872\n",
      "Epoch: 067, Loss: 0.5688, Train Acc: 0.6955\n",
      "Epoch: 068, Loss: 0.5681, Train Acc: 0.6872\n",
      "Epoch: 069, Loss: 0.5678, Train Acc: 0.6914\n",
      "Epoch: 070, Loss: 0.5665, Train Acc: 0.6872\n",
      "Epoch: 071, Loss: 0.5645, Train Acc: 0.6831\n",
      "Epoch: 072, Loss: 0.5629, Train Acc: 0.7037\n",
      "Epoch: 073, Loss: 0.5621, Train Acc: 0.7037\n",
      "Epoch: 074, Loss: 0.5610, Train Acc: 0.7078\n",
      "Epoch: 075, Loss: 0.5590, Train Acc: 0.6831\n",
      "Epoch: 076, Loss: 0.5575, Train Acc: 0.6914\n",
      "Epoch: 077, Loss: 0.5564, Train Acc: 0.6914\n",
      "Epoch: 078, Loss: 0.5551, Train Acc: 0.6914\n",
      "Epoch: 079, Loss: 0.5534, Train Acc: 0.7037\n",
      "Epoch: 080, Loss: 0.5518, Train Acc: 0.7243\n",
      "Epoch: 081, Loss: 0.5506, Train Acc: 0.7243\n",
      "Epoch: 082, Loss: 0.5491, Train Acc: 0.7037\n",
      "Epoch: 083, Loss: 0.5473, Train Acc: 0.6955\n",
      "Epoch: 084, Loss: 0.5459, Train Acc: 0.6996\n",
      "Epoch: 085, Loss: 0.5444, Train Acc: 0.6996\n",
      "Epoch: 086, Loss: 0.5426, Train Acc: 0.7202\n",
      "Epoch: 087, Loss: 0.5409, Train Acc: 0.7284\n",
      "Epoch: 088, Loss: 0.5394, Train Acc: 0.7284\n",
      "Epoch: 089, Loss: 0.5375, Train Acc: 0.7119\n",
      "Epoch: 090, Loss: 0.5357, Train Acc: 0.7119\n",
      "Epoch: 091, Loss: 0.5341, Train Acc: 0.7202\n",
      "Epoch: 092, Loss: 0.5321, Train Acc: 0.7366\n",
      "Epoch: 093, Loss: 0.5305, Train Acc: 0.7366\n",
      "Epoch: 094, Loss: 0.5285, Train Acc: 0.7325\n",
      "Epoch: 095, Loss: 0.5264, Train Acc: 0.7325\n",
      "Epoch: 096, Loss: 0.5248, Train Acc: 0.7407\n",
      "Epoch: 097, Loss: 0.5224, Train Acc: 0.7449\n",
      "Epoch: 098, Loss: 0.5205, Train Acc: 0.7407\n",
      "Epoch: 099, Loss: 0.5183, Train Acc: 0.7490\n",
      "Epoch: 100, Loss: 0.5161, Train Acc: 0.7449\n",
      "Average Train Precision: 0.7136, Average Train Recall: 0.5999, Average Train F1: 0.5584\n",
      "Average Val Precision: 0.7141, Average Val Recall: 0.6470, Average Val F1: 0.6175\n",
      "Test Acc: 0.6176, Test Precision: 0.5443, Test Recall: 0.5219, Test F1: 0.4902\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'targets', 'target'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'isDoping', 'doping'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'interactsWith', 'drug'): SAGEConv(-1, hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Instantiate the model\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "\n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "\n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(data)\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(data, data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(data, data['drug'].val_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(data, data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 7.1550, Train Acc: 0.3868\n",
      "Epoch: 002, Loss: 201.6221, Train Acc: 0.3868\n",
      "Epoch: 003, Loss: 82.9820, Train Acc: 0.4156\n",
      "Epoch: 004, Loss: 2.4089, Train Acc: 0.6173\n",
      "Epoch: 005, Loss: 35.3436, Train Acc: 0.6173\n",
      "Epoch: 006, Loss: 36.5569, Train Acc: 0.6173\n",
      "Epoch: 007, Loss: 29.8772, Train Acc: 0.6091\n",
      "Epoch: 008, Loss: 19.5826, Train Acc: 0.6296\n",
      "Epoch: 009, Loss: 7.8115, Train Acc: 0.4074\n",
      "Epoch: 010, Loss: 6.9310, Train Acc: 0.3827\n",
      "Epoch: 011, Loss: 7.9197, Train Acc: 0.4074\n",
      "Epoch: 012, Loss: 4.8197, Train Acc: 0.5967\n",
      "Epoch: 013, Loss: 2.9594, Train Acc: 0.6214\n",
      "Epoch: 014, Loss: 3.7908, Train Acc: 0.6173\n",
      "Epoch: 015, Loss: 3.2624, Train Acc: 0.5885\n",
      "Epoch: 016, Loss: 1.9963, Train Acc: 0.4444\n",
      "Epoch: 017, Loss: 1.8125, Train Acc: 0.4280\n",
      "Epoch: 018, Loss: 2.1992, Train Acc: 0.4650\n",
      "Epoch: 019, Loss: 1.3626, Train Acc: 0.6173\n",
      "Epoch: 020, Loss: 1.3274, Train Acc: 0.6091\n",
      "Epoch: 021, Loss: 1.5787, Train Acc: 0.6173\n",
      "Epoch: 022, Loss: 1.2790, Train Acc: 0.5926\n",
      "Epoch: 023, Loss: 0.8653, Train Acc: 0.4486\n",
      "Epoch: 024, Loss: 1.1955, Train Acc: 0.4568\n",
      "Epoch: 025, Loss: 0.9830, Train Acc: 0.6255\n",
      "Epoch: 026, Loss: 0.8931, Train Acc: 0.6214\n",
      "Epoch: 027, Loss: 0.7904, Train Acc: 0.6337\n",
      "Epoch: 028, Loss: 0.6842, Train Acc: 0.5432\n",
      "Epoch: 029, Loss: 0.7237, Train Acc: 0.6132\n",
      "Epoch: 030, Loss: 0.6998, Train Acc: 0.6831\n",
      "Epoch: 031, Loss: 0.6852, Train Acc: 0.6667\n",
      "Epoch: 032, Loss: 0.7112, Train Acc: 0.6626\n",
      "Epoch: 033, Loss: 0.6703, Train Acc: 0.6461\n",
      "Epoch: 034, Loss: 0.6546, Train Acc: 0.6296\n",
      "Epoch: 035, Loss: 0.6647, Train Acc: 0.6584\n",
      "Epoch: 036, Loss: 0.6190, Train Acc: 0.6626\n",
      "Epoch: 037, Loss: 0.6284, Train Acc: 0.6543\n",
      "Epoch: 038, Loss: 0.6179, Train Acc: 0.6584\n",
      "Epoch: 039, Loss: 0.6059, Train Acc: 0.5761\n",
      "Epoch: 040, Loss: 0.6260, Train Acc: 0.6584\n",
      "Epoch: 041, Loss: 0.5991, Train Acc: 0.6420\n",
      "Epoch: 042, Loss: 0.6113, Train Acc: 0.6708\n",
      "Epoch: 043, Loss: 0.5941, Train Acc: 0.6996\n",
      "Epoch: 044, Loss: 0.5907, Train Acc: 0.6955\n",
      "Epoch: 045, Loss: 0.5899, Train Acc: 0.6872\n",
      "Epoch: 046, Loss: 0.5828, Train Acc: 0.6790\n",
      "Epoch: 047, Loss: 0.5917, Train Acc: 0.6831\n",
      "Epoch: 048, Loss: 0.5800, Train Acc: 0.7119\n",
      "Epoch: 049, Loss: 0.5871, Train Acc: 0.7037\n",
      "Epoch: 050, Loss: 0.5750, Train Acc: 0.6749\n",
      "Epoch: 051, Loss: 0.5785, Train Acc: 0.6790\n",
      "Epoch: 052, Loss: 0.5717, Train Acc: 0.7160\n",
      "Epoch: 053, Loss: 0.5698, Train Acc: 0.7160\n",
      "Epoch: 054, Loss: 0.5669, Train Acc: 0.6955\n",
      "Epoch: 055, Loss: 0.5619, Train Acc: 0.6996\n",
      "Epoch: 056, Loss: 0.5623, Train Acc: 0.7037\n",
      "Epoch: 057, Loss: 0.5581, Train Acc: 0.7160\n",
      "Epoch: 058, Loss: 0.5609, Train Acc: 0.7119\n",
      "Epoch: 059, Loss: 0.5554, Train Acc: 0.7078\n",
      "Epoch: 060, Loss: 0.5561, Train Acc: 0.7078\n",
      "Epoch: 061, Loss: 0.5512, Train Acc: 0.7325\n",
      "Epoch: 062, Loss: 0.5519, Train Acc: 0.7078\n",
      "Epoch: 063, Loss: 0.5489, Train Acc: 0.7037\n",
      "Epoch: 064, Loss: 0.5495, Train Acc: 0.7037\n",
      "Epoch: 065, Loss: 0.5467, Train Acc: 0.7202\n",
      "Epoch: 066, Loss: 0.5466, Train Acc: 0.7119\n",
      "Epoch: 067, Loss: 0.5441, Train Acc: 0.7078\n",
      "Epoch: 068, Loss: 0.5439, Train Acc: 0.7119\n",
      "Epoch: 069, Loss: 0.5416, Train Acc: 0.7284\n",
      "Epoch: 070, Loss: 0.5414, Train Acc: 0.7119\n",
      "Epoch: 071, Loss: 0.5392, Train Acc: 0.7202\n",
      "Epoch: 072, Loss: 0.5391, Train Acc: 0.7243\n",
      "Epoch: 073, Loss: 0.5375, Train Acc: 0.7284\n",
      "Epoch: 074, Loss: 0.5375, Train Acc: 0.7243\n",
      "Epoch: 075, Loss: 0.5358, Train Acc: 0.7202\n",
      "Epoch: 076, Loss: 0.5356, Train Acc: 0.7243\n",
      "Epoch: 077, Loss: 0.5340, Train Acc: 0.7284\n",
      "Epoch: 078, Loss: 0.5337, Train Acc: 0.7202\n",
      "Epoch: 079, Loss: 0.5325, Train Acc: 0.7202\n",
      "Epoch: 080, Loss: 0.5321, Train Acc: 0.7325\n",
      "Epoch: 081, Loss: 0.5312, Train Acc: 0.7284\n",
      "Epoch: 082, Loss: 0.5302, Train Acc: 0.7284\n",
      "Epoch: 083, Loss: 0.5297, Train Acc: 0.7243\n",
      "Epoch: 084, Loss: 0.5285, Train Acc: 0.7243\n",
      "Epoch: 085, Loss: 0.5280, Train Acc: 0.7243\n",
      "Epoch: 086, Loss: 0.5268, Train Acc: 0.7243\n",
      "Epoch: 087, Loss: 0.5265, Train Acc: 0.7243\n",
      "Epoch: 088, Loss: 0.5258, Train Acc: 0.7202\n",
      "Epoch: 089, Loss: 0.5249, Train Acc: 0.7202\n",
      "Epoch: 090, Loss: 0.5246, Train Acc: 0.7160\n",
      "Epoch: 091, Loss: 0.5236, Train Acc: 0.7202\n",
      "Epoch: 092, Loss: 0.5225, Train Acc: 0.7160\n",
      "Epoch: 093, Loss: 0.5221, Train Acc: 0.7202\n",
      "Epoch: 094, Loss: 0.5213, Train Acc: 0.7202\n",
      "Epoch: 095, Loss: 0.5205, Train Acc: 0.7160\n",
      "Epoch: 096, Loss: 0.5199, Train Acc: 0.7160\n",
      "Epoch: 097, Loss: 0.5192, Train Acc: 0.7160\n",
      "Epoch: 098, Loss: 0.5185, Train Acc: 0.7160\n",
      "Epoch: 099, Loss: 0.5178, Train Acc: 0.7119\n",
      "Epoch: 100, Loss: 0.5170, Train Acc: 0.7160\n",
      "Average Train Precision: 0.6630, Average Train Recall: 0.6108, Average Train F1: 0.5839\n",
      "Average Val Precision: 0.6398, Average Val Recall: 0.5760, Average Val F1: 0.5471\n",
      "Test Acc: 0.5735, Test Precision: 0.4792, Test Recall: 0.4870, Test F1: 0.4616\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads=1):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True,add_self_loops = False),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops = False),\n",
    "            ('drug', 'targets', 'target'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops = False),\n",
    "            ('drug', 'isDoping', 'doping'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops = False),\n",
    "            ('drug', 'interactsWith', 'drug'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops = False),\n",
    "        }, aggr='sum')\n",
    "        \n",
    "        # If concat=True in GATConv, the output channels are multiplied by the number of heads\n",
    "        gat_output_channels = hidden_channels * num_heads\n",
    "        \n",
    "        self.lin = Linear(gat_output_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Instantiate the model\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2, num_heads=8)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "\n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "\n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(data)\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(data, data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(data, data['drug'].val_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(data, data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6009] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1604] },\n",
      "  (drug, targets, target)={ edge_index=[2, 243] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 243] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46111] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 668] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 179] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5124] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1670] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 446] },\n",
      "  (drug, targets, target)={ edge_index=[2, 68] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 68] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12809] }\n",
      ")\n",
      "Epoch: 001, Loss: 17.2946, Train Acc: 0.6132\n",
      "Epoch: 002, Loss: 55.3998, Train Acc: 0.6132\n",
      "Epoch: 003, Loss: 19.0637, Train Acc: 0.6132\n",
      "Epoch: 004, Loss: 11.4744, Train Acc: 0.6132\n",
      "Epoch: 005, Loss: 4.3903, Train Acc: 0.3868\n",
      "Epoch: 006, Loss: 6.2483, Train Acc: 0.3868\n",
      "Epoch: 007, Loss: 6.5423, Train Acc: 0.4815\n",
      "Epoch: 008, Loss: 2.8071, Train Acc: 0.6132\n",
      "Epoch: 009, Loss: 2.9383, Train Acc: 0.6132\n",
      "Epoch: 010, Loss: 3.9063, Train Acc: 0.6255\n",
      "Epoch: 011, Loss: 2.7623, Train Acc: 0.5021\n",
      "Epoch: 012, Loss: 2.0455, Train Acc: 0.3909\n",
      "Epoch: 013, Loss: 3.0870, Train Acc: 0.4691\n",
      "Epoch: 014, Loss: 2.4365, Train Acc: 0.5021\n",
      "Epoch: 015, Loss: 1.5947, Train Acc: 0.6255\n",
      "Epoch: 016, Loss: 2.1893, Train Acc: 0.6214\n",
      "Epoch: 017, Loss: 2.2953, Train Acc: 0.6379\n",
      "Epoch: 018, Loss: 1.4322, Train Acc: 0.4321\n",
      "Epoch: 019, Loss: 1.8014, Train Acc: 0.4321\n",
      "Epoch: 020, Loss: 1.7084, Train Acc: 0.6379\n",
      "Epoch: 021, Loss: 1.0607, Train Acc: 0.6461\n",
      "Epoch: 022, Loss: 1.4554, Train Acc: 0.6420\n",
      "Epoch: 023, Loss: 1.2001, Train Acc: 0.5638\n",
      "Epoch: 024, Loss: 0.8242, Train Acc: 0.4156\n",
      "Epoch: 025, Loss: 1.2241, Train Acc: 0.6173\n",
      "Epoch: 026, Loss: 0.7094, Train Acc: 0.6255\n",
      "Epoch: 027, Loss: 1.0612, Train Acc: 0.6337\n",
      "Epoch: 028, Loss: 0.8804, Train Acc: 0.5103\n",
      "Epoch: 029, Loss: 0.8171, Train Acc: 0.4897\n",
      "Epoch: 030, Loss: 0.9201, Train Acc: 0.6626\n",
      "Epoch: 031, Loss: 0.7367, Train Acc: 0.6420\n",
      "Epoch: 032, Loss: 0.8487, Train Acc: 0.6461\n",
      "Epoch: 033, Loss: 0.6509, Train Acc: 0.5350\n",
      "Epoch: 034, Loss: 0.7699, Train Acc: 0.6296\n",
      "Epoch: 035, Loss: 0.6350, Train Acc: 0.6502\n",
      "Epoch: 036, Loss: 0.6968, Train Acc: 0.6667\n",
      "Epoch: 037, Loss: 0.6634, Train Acc: 0.6337\n",
      "Epoch: 038, Loss: 0.6262, Train Acc: 0.5720\n",
      "Epoch: 039, Loss: 0.6613, Train Acc: 0.6914\n",
      "Epoch: 040, Loss: 0.6062, Train Acc: 0.6708\n",
      "Epoch: 041, Loss: 0.6597, Train Acc: 0.7243\n",
      "Epoch: 042, Loss: 0.5930, Train Acc: 0.5761\n",
      "Epoch: 043, Loss: 0.6521, Train Acc: 0.7202\n",
      "Epoch: 044, Loss: 0.5860, Train Acc: 0.6749\n",
      "Epoch: 045, Loss: 0.6249, Train Acc: 0.6996\n",
      "Epoch: 046, Loss: 0.5817, Train Acc: 0.6461\n",
      "Epoch: 047, Loss: 0.6066, Train Acc: 0.6708\n",
      "Epoch: 048, Loss: 0.5781, Train Acc: 0.6996\n",
      "Epoch: 049, Loss: 0.5948, Train Acc: 0.7037\n",
      "Epoch: 050, Loss: 0.5817, Train Acc: 0.6708\n",
      "Epoch: 051, Loss: 0.5846, Train Acc: 0.6872\n",
      "Epoch: 052, Loss: 0.5778, Train Acc: 0.6996\n",
      "Epoch: 053, Loss: 0.5764, Train Acc: 0.6996\n",
      "Epoch: 054, Loss: 0.5750, Train Acc: 0.7160\n",
      "Epoch: 055, Loss: 0.5683, Train Acc: 0.7284\n",
      "Epoch: 056, Loss: 0.5700, Train Acc: 0.7078\n",
      "Epoch: 057, Loss: 0.5633, Train Acc: 0.6955\n",
      "Epoch: 058, Loss: 0.5654, Train Acc: 0.7325\n",
      "Epoch: 059, Loss: 0.5584, Train Acc: 0.7490\n",
      "Epoch: 060, Loss: 0.5618, Train Acc: 0.7202\n",
      "Epoch: 061, Loss: 0.5568, Train Acc: 0.7202\n",
      "Epoch: 062, Loss: 0.5592, Train Acc: 0.7366\n",
      "Epoch: 063, Loss: 0.5555, Train Acc: 0.7325\n",
      "Epoch: 064, Loss: 0.5551, Train Acc: 0.7243\n",
      "Epoch: 065, Loss: 0.5536, Train Acc: 0.7202\n",
      "Epoch: 066, Loss: 0.5526, Train Acc: 0.7407\n",
      "Epoch: 067, Loss: 0.5520, Train Acc: 0.7531\n",
      "Epoch: 068, Loss: 0.5503, Train Acc: 0.7202\n",
      "Epoch: 069, Loss: 0.5504, Train Acc: 0.7243\n",
      "Epoch: 070, Loss: 0.5483, Train Acc: 0.7449\n",
      "Epoch: 071, Loss: 0.5491, Train Acc: 0.7407\n",
      "Epoch: 072, Loss: 0.5465, Train Acc: 0.7202\n",
      "Epoch: 073, Loss: 0.5481, Train Acc: 0.7243\n",
      "Epoch: 074, Loss: 0.5450, Train Acc: 0.7407\n",
      "Epoch: 075, Loss: 0.5466, Train Acc: 0.7284\n",
      "Epoch: 076, Loss: 0.5436, Train Acc: 0.7243\n",
      "Epoch: 077, Loss: 0.5449, Train Acc: 0.7325\n",
      "Epoch: 078, Loss: 0.5424, Train Acc: 0.7407\n",
      "Epoch: 079, Loss: 0.5432, Train Acc: 0.7243\n",
      "Epoch: 080, Loss: 0.5415, Train Acc: 0.7243\n",
      "Epoch: 081, Loss: 0.5413, Train Acc: 0.7407\n",
      "Epoch: 082, Loss: 0.5401, Train Acc: 0.7366\n",
      "Epoch: 083, Loss: 0.5392, Train Acc: 0.7243\n",
      "Epoch: 084, Loss: 0.5389, Train Acc: 0.7284\n",
      "Epoch: 085, Loss: 0.5375, Train Acc: 0.7407\n",
      "Epoch: 086, Loss: 0.5376, Train Acc: 0.7366\n",
      "Epoch: 087, Loss: 0.5361, Train Acc: 0.7325\n",
      "Epoch: 088, Loss: 0.5359, Train Acc: 0.7449\n",
      "Epoch: 089, Loss: 0.5346, Train Acc: 0.7449\n",
      "Epoch: 090, Loss: 0.5338, Train Acc: 0.7325\n",
      "Epoch: 091, Loss: 0.5332, Train Acc: 0.7407\n",
      "Epoch: 092, Loss: 0.5317, Train Acc: 0.7490\n",
      "Epoch: 093, Loss: 0.5315, Train Acc: 0.7325\n",
      "Epoch: 094, Loss: 0.5301, Train Acc: 0.7366\n",
      "Epoch: 095, Loss: 0.5289, Train Acc: 0.7490\n",
      "Epoch: 096, Loss: 0.5282, Train Acc: 0.7366\n",
      "Epoch: 097, Loss: 0.5267, Train Acc: 0.7407\n",
      "Epoch: 098, Loss: 0.5255, Train Acc: 0.7449\n",
      "Epoch: 099, Loss: 0.5247, Train Acc: 0.7407\n",
      "Epoch: 100, Loss: 0.5234, Train Acc: 0.7449\n",
      "Average Train Precision: 0.6719, Average Train Recall: 0.6166, Average Train F1: 0.5897\n",
      "Average Val Precision: 0.5344, Average Val Recall: 0.5434, Average Val F1: 0.5171\n",
      "Test Acc: 0.5294, Test Precision: 0.4850, Test Recall: 0.4856, Test F1: 0.4848\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GATConv, Linear\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume data is your input HeteroData\n",
    "data = ToUndirected()(data_small)\n",
    "\n",
    "# Create labels for the 'drug' nodes based on their connections to 'doping'\n",
    "doping_labels = torch.zeros(data['drug'].x.size(0), dtype=torch.long)\n",
    "\n",
    "# Assuming encoded_doping_df has the doping information\n",
    "# 0 means not doping, 1 means is doping\n",
    "for drug_idx, doping in enumerate(encoded_doping_df['Doping']):\n",
    "    if doping == 1:\n",
    "        doping_labels[drug_idx] = 1\n",
    "\n",
    "data['drug'].y = doping_labels\n",
    "\n",
    "def split_edges(data, edge_type, test_size=0.2, val_size=0.1):\n",
    "    edge_index = data[edge_type].edge_index.numpy()\n",
    "    num_edges = edge_index.shape[1]\n",
    "    \n",
    "    # Split the edges into training and testing sets\n",
    "    train_edges, test_edges = train_test_split(range(num_edges), test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Further split the training edges into training and validation sets\n",
    "    train_edges, val_edges = train_test_split(train_edges, test_size=val_size, random_state=42)\n",
    "    \n",
    "    # Create edge indices\n",
    "    train_edge_index = torch.tensor(edge_index[:, train_edges], dtype=torch.long)\n",
    "    val_edge_index = torch.tensor(edge_index[:, val_edges], dtype=torch.long)\n",
    "    test_edge_index = torch.tensor(edge_index[:, test_edges], dtype=torch.long)\n",
    "    \n",
    "    return train_edge_index, val_edge_index, test_edge_index\n",
    "\n",
    "# Define edge types\n",
    "edge_types = [\n",
    "    ('drug', 'isInCategory', 'drug_category'),\n",
    "    ('drug', 'isClassifiedAs', 'atc_code'),\n",
    "    ('drug', 'targets', 'target'),\n",
    "    ('drug', 'isDoping', 'doping'),\n",
    "    ('drug', 'interactsWith', 'drug')\n",
    "]\n",
    "\n",
    "# Initialize the train, validation, and test data\n",
    "train_data = HeteroData()\n",
    "val_data = HeteroData()\n",
    "test_data = HeteroData()\n",
    "\n",
    "for edge_type in edge_types:\n",
    "    train_edge_index, val_edge_index, test_edge_index = split_edges(data, edge_type)\n",
    "    \n",
    "    train_data[edge_type].edge_index = train_edge_index\n",
    "    val_data[edge_type].edge_index = val_edge_index\n",
    "    test_data[edge_type].edge_index = test_edge_index\n",
    "    \n",
    "    if 'x' in data[edge_type[0]]:\n",
    "        train_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        val_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        test_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        \n",
    "    if 'x' in data[edge_type[2]]:\n",
    "        train_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "        val_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "        test_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "\n",
    "# Set the node features and labels for the 'drug' nodes in the train, validation, and test data\n",
    "num_nodes = data['drug'].x.size(0)\n",
    "train_data['drug'].x = data['drug'].x\n",
    "train_data['drug'].y = data['drug'].y\n",
    "val_data['drug'].x = data['drug'].x\n",
    "val_data['drug'].y = data['drug'].y\n",
    "test_data['drug'].x = data['drug'].x\n",
    "test_data['drug'].y = data['drug'].y\n",
    "\n",
    "# Create train, validation, and test masks for nodes\n",
    "def create_node_masks(data, num_nodes, train_ratio=0.8, val_ratio=0.1):\n",
    "    train_mask, test_mask = train_test_split(range(num_nodes), test_size=1-train_ratio, random_state=42)\n",
    "    train_mask, val_mask = train_test_split(train_mask, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    mask_dict = {\n",
    "        'train_mask': torch.zeros(num_nodes, dtype=torch.bool),\n",
    "        'val_mask': torch.zeros(num_nodes, dtype=torch.bool),\n",
    "        'test_mask': torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    }\n",
    "    \n",
    "    mask_dict['train_mask'][train_mask] = True\n",
    "    mask_dict['val_mask'][val_mask] = True\n",
    "    mask_dict['test_mask'][test_mask] = True\n",
    "    \n",
    "    return mask_dict\n",
    "\n",
    "node_masks = create_node_masks(data, num_nodes)\n",
    "\n",
    "train_data['drug'].train_mask = node_masks['train_mask']\n",
    "train_data['drug'].val_mask = node_masks['val_mask']\n",
    "train_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "val_data['drug'].train_mask = node_masks['train_mask']\n",
    "val_data['drug'].val_mask = node_masks['val_mask']\n",
    "val_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "test_data['drug'].train_mask = node_masks['train_mask']\n",
    "test_data['drug'].val_mask = node_masks['val_mask']\n",
    "test_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "# Print data to verify the splits\n",
    "print(\"Train Data:\", train_data)\n",
    "print(\"Validation Data:\", val_data)\n",
    "print(\"Test Data:\", test_data)\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads=1):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops=False),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops=False),\n",
    "            ('drug', 'targets', 'target'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops=False),\n",
    "            ('drug', 'isDoping', 'doping'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops=False),\n",
    "            ('drug', 'interactsWith', 'drug'): GATConv((-1, -1), hidden_channels, heads=num_heads, concat=True, add_self_loops=False),\n",
    "        }, aggr='sum')\n",
    "        \n",
    "        # If concat=True in GATConv, the output channels are multiplied by the number of heads\n",
    "        gat_output_channels = hidden_channels * num_heads\n",
    "        \n",
    "        self.lin = Linear(gat_output_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2, num_heads=8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "\n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "\n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_data)\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(train_data, train_data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(val_data, val_data['drug'].val_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(test_data, test_data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6009] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1604] },\n",
      "  (drug, targets, target)={ edge_index=[2, 243] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 243] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46111] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 668] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 179] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5124] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1670] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 446] },\n",
      "  (drug, targets, target)={ edge_index=[2, 68] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 68] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12809] }\n",
      ")\n",
      "Epoch: 001, Loss: 583.0455, Train Acc: 0.6132\n",
      "Epoch: 002, Loss: 481.3088, Train Acc: 0.6173\n",
      "Epoch: 003, Loss: 418.0832, Train Acc: 0.6173\n",
      "Epoch: 004, Loss: 173.4587, Train Acc: 0.3868\n",
      "Epoch: 005, Loss: 299.7402, Train Acc: 0.3868\n",
      "Epoch: 006, Loss: 101.5888, Train Acc: 0.6173\n",
      "Epoch: 007, Loss: 179.5942, Train Acc: 0.6173\n",
      "Epoch: 008, Loss: 251.2524, Train Acc: 0.6173\n",
      "Epoch: 009, Loss: 224.1252, Train Acc: 0.6173\n",
      "Epoch: 010, Loss: 136.5421, Train Acc: 0.6214\n",
      "Epoch: 011, Loss: 13.7566, Train Acc: 0.3868\n",
      "Epoch: 012, Loss: 218.4523, Train Acc: 0.3868\n",
      "Epoch: 013, Loss: 279.1537, Train Acc: 0.3868\n",
      "Epoch: 014, Loss: 222.8405, Train Acc: 0.3868\n",
      "Epoch: 015, Loss: 91.5789, Train Acc: 0.6173\n",
      "Epoch: 016, Loss: 48.0039, Train Acc: 0.6173\n",
      "Epoch: 017, Loss: 108.5532, Train Acc: 0.6173\n",
      "Epoch: 018, Loss: 134.6903, Train Acc: 0.6173\n",
      "Epoch: 019, Loss: 132.4130, Train Acc: 0.6173\n",
      "Epoch: 020, Loss: 110.7132, Train Acc: 0.6173\n",
      "Epoch: 021, Loss: 76.6091, Train Acc: 0.6132\n",
      "Epoch: 022, Loss: 35.3575, Train Acc: 0.3868\n",
      "Epoch: 023, Loss: 12.8230, Train Acc: 0.3868\n",
      "Epoch: 024, Loss: 43.0161, Train Acc: 0.3868\n",
      "Epoch: 025, Loss: 39.6430, Train Acc: 0.3868\n",
      "Epoch: 026, Loss: 11.7416, Train Acc: 0.6173\n",
      "Epoch: 027, Loss: 18.6316, Train Acc: 0.6173\n",
      "Epoch: 028, Loss: 34.2692, Train Acc: 0.6173\n",
      "Epoch: 029, Loss: 40.7650, Train Acc: 0.6173\n",
      "Epoch: 030, Loss: 39.6699, Train Acc: 0.6173\n",
      "Epoch: 031, Loss: 32.7396, Train Acc: 0.6173\n",
      "Epoch: 032, Loss: 21.4465, Train Acc: 0.6337\n",
      "Epoch: 033, Loss: 7.4737, Train Acc: 0.3868\n",
      "Epoch: 034, Loss: 13.7185, Train Acc: 0.3868\n",
      "Epoch: 035, Loss: 24.1849, Train Acc: 0.3868\n",
      "Epoch: 036, Loss: 21.1684, Train Acc: 0.3868\n",
      "Epoch: 037, Loss: 7.7200, Train Acc: 0.6337\n",
      "Epoch: 038, Loss: 7.1145, Train Acc: 0.6173\n",
      "Epoch: 039, Loss: 13.7607, Train Acc: 0.6132\n",
      "Epoch: 040, Loss: 16.1625, Train Acc: 0.6132\n",
      "Epoch: 041, Loss: 15.6513, Train Acc: 0.6173\n",
      "Epoch: 042, Loss: 12.8994, Train Acc: 0.6214\n",
      "Epoch: 043, Loss: 8.5232, Train Acc: 0.6379\n",
      "Epoch: 044, Loss: 3.1849, Train Acc: 0.3868\n",
      "Epoch: 045, Loss: 4.8847, Train Acc: 0.3868\n",
      "Epoch: 046, Loss: 9.0842, Train Acc: 0.3868\n",
      "Epoch: 047, Loss: 8.1338, Train Acc: 0.3868\n",
      "Epoch: 048, Loss: 3.1898, Train Acc: 0.6337\n",
      "Epoch: 049, Loss: 2.6025, Train Acc: 0.6296\n",
      "Epoch: 050, Loss: 5.2389, Train Acc: 0.6214\n",
      "Epoch: 051, Loss: 6.4406, Train Acc: 0.6214\n",
      "Epoch: 052, Loss: 6.3601, Train Acc: 0.6255\n",
      "Epoch: 053, Loss: 5.2642, Train Acc: 0.6337\n",
      "Epoch: 054, Loss: 3.4561, Train Acc: 0.6091\n",
      "Epoch: 055, Loss: 1.3427, Train Acc: 0.3868\n",
      "Epoch: 056, Loss: 2.1920, Train Acc: 0.3868\n",
      "Epoch: 057, Loss: 3.8133, Train Acc: 0.3868\n",
      "Epoch: 058, Loss: 3.2499, Train Acc: 0.4156\n",
      "Epoch: 059, Loss: 1.2173, Train Acc: 0.6049\n",
      "Epoch: 060, Loss: 1.1972, Train Acc: 0.6296\n",
      "Epoch: 061, Loss: 2.0962, Train Acc: 0.6379\n",
      "Epoch: 062, Loss: 2.4962, Train Acc: 0.6379\n",
      "Epoch: 063, Loss: 2.4204, Train Acc: 0.6337\n",
      "Epoch: 064, Loss: 1.9813, Train Acc: 0.6337\n",
      "Epoch: 065, Loss: 1.3243, Train Acc: 0.6255\n",
      "Epoch: 066, Loss: 0.7336, Train Acc: 0.4033\n",
      "Epoch: 067, Loss: 0.9616, Train Acc: 0.3868\n",
      "Epoch: 068, Loss: 1.4739, Train Acc: 0.3868\n",
      "Epoch: 069, Loss: 1.4236, Train Acc: 0.3909\n",
      "Epoch: 070, Loss: 0.9672, Train Acc: 0.6049\n",
      "Epoch: 071, Loss: 0.6553, Train Acc: 0.6132\n",
      "Epoch: 072, Loss: 0.7526, Train Acc: 0.6379\n",
      "Epoch: 073, Loss: 0.9229, Train Acc: 0.6379\n",
      "Epoch: 074, Loss: 1.0063, Train Acc: 0.6379\n",
      "Epoch: 075, Loss: 0.9867, Train Acc: 0.6420\n",
      "Epoch: 076, Loss: 0.8918, Train Acc: 0.6296\n",
      "Epoch: 077, Loss: 0.7675, Train Acc: 0.6132\n",
      "Epoch: 078, Loss: 0.6683, Train Acc: 0.6337\n",
      "Epoch: 079, Loss: 0.6381, Train Acc: 0.5391\n",
      "Epoch: 080, Loss: 0.6742, Train Acc: 0.4033\n",
      "Epoch: 081, Loss: 0.7287, Train Acc: 0.3868\n",
      "Epoch: 082, Loss: 0.7581, Train Acc: 0.3868\n",
      "Epoch: 083, Loss: 0.7507, Train Acc: 0.3992\n",
      "Epoch: 084, Loss: 0.7184, Train Acc: 0.5391\n",
      "Epoch: 085, Loss: 0.6805, Train Acc: 0.5967\n",
      "Epoch: 086, Loss: 0.6523, Train Acc: 0.6296\n",
      "Epoch: 087, Loss: 0.6397, Train Acc: 0.6214\n",
      "Epoch: 088, Loss: 0.6405, Train Acc: 0.6214\n",
      "Epoch: 089, Loss: 0.6487, Train Acc: 0.6296\n",
      "Epoch: 090, Loss: 0.6585, Train Acc: 0.6379\n",
      "Epoch: 091, Loss: 0.6658, Train Acc: 0.6420\n",
      "Epoch: 092, Loss: 0.6691, Train Acc: 0.6420\n",
      "Epoch: 093, Loss: 0.6681, Train Acc: 0.6420\n",
      "Epoch: 094, Loss: 0.6636, Train Acc: 0.6379\n",
      "Epoch: 095, Loss: 0.6575, Train Acc: 0.6379\n",
      "Epoch: 096, Loss: 0.6515, Train Acc: 0.6296\n",
      "Epoch: 097, Loss: 0.6465, Train Acc: 0.6214\n",
      "Epoch: 098, Loss: 0.6432, Train Acc: 0.6091\n",
      "Epoch: 099, Loss: 0.6420, Train Acc: 0.6214\n",
      "Epoch: 100, Loss: 0.6425, Train Acc: 0.6255\n",
      "Average Train Precision: 0.5427, Average Train Recall: 0.5201, Average Train F1: 0.4043\n",
      "Average Val Precision: 0.3377, Average Val Recall: 0.5299, Average Val F1: 0.3625\n",
      "Test Acc: 0.4559, Test Precision: 0.5983, Test Recall: 0.5530, Test F1: 0.4281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, GraphConv, Linear\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume data is your input HeteroData\n",
    "data = ToUndirected()(data_small)\n",
    "\n",
    "# Create labels for the 'drug' nodes based on their connections to 'doping'\n",
    "doping_labels = torch.zeros(data['drug'].x.size(0), dtype=torch.long)\n",
    "\n",
    "# Assuming encoded_doping_df has the doping information\n",
    "# 0 means not doping, 1 means is doping\n",
    "for drug_idx, doping in enumerate(encoded_doping_df['Doping']):\n",
    "    if doping == 1:\n",
    "        doping_labels[drug_idx] = 1\n",
    "\n",
    "data['drug'].y = doping_labels\n",
    "\n",
    "def split_edges(data, edge_type, test_size=0.2, val_size=0.1):\n",
    "    edge_index = data[edge_type].edge_index.numpy()\n",
    "    num_edges = edge_index.shape[1]\n",
    "    \n",
    "    # Split the edges into training and testing sets\n",
    "    train_edges, test_edges = train_test_split(range(num_edges), test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Further split the training edges into training and validation sets\n",
    "    train_edges, val_edges = train_test_split(train_edges, test_size=val_size, random_state=42)\n",
    "    \n",
    "    # Create edge indices\n",
    "    train_edge_index = torch.tensor(edge_index[:, train_edges], dtype=torch.long)\n",
    "    val_edge_index = torch.tensor(edge_index[:, val_edges], dtype=torch.long)\n",
    "    test_edge_index = torch.tensor(edge_index[:, test_edges], dtype=torch.long)\n",
    "    \n",
    "    return train_edge_index, val_edge_index, test_edge_index\n",
    "\n",
    "# Define edge types\n",
    "edge_types = [\n",
    "    ('drug', 'isInCategory', 'drug_category'),\n",
    "    ('drug', 'isClassifiedAs', 'atc_code'),\n",
    "    ('drug', 'targets', 'target'),\n",
    "    ('drug', 'isDoping', 'doping'),\n",
    "    ('drug', 'interactsWith', 'drug')\n",
    "]\n",
    "\n",
    "# Initialize the train, validation, and test data\n",
    "train_data = HeteroData()\n",
    "val_data = HeteroData()\n",
    "test_data = HeteroData()\n",
    "\n",
    "for edge_type in edge_types:\n",
    "    train_edge_index, val_edge_index, test_edge_index = split_edges(data, edge_type)\n",
    "    \n",
    "    train_data[edge_type].edge_index = train_edge_index\n",
    "    val_data[edge_type].edge_index = val_edge_index\n",
    "    test_data[edge_type].edge_index = test_edge_index\n",
    "    \n",
    "    if 'x' in data[edge_type[0]]:\n",
    "        train_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        val_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        test_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        \n",
    "    if 'x' in data[edge_type[2]]:\n",
    "        train_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "        val_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "        test_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "\n",
    "# Set the node features and labels for the 'drug' nodes in the train, validation, and test data\n",
    "num_nodes = data['drug'].x.size(0)\n",
    "train_data['drug'].x = data['drug'].x\n",
    "train_data['drug'].y = data['drug'].y\n",
    "val_data['drug'].x = data['drug'].x\n",
    "val_data['drug'].y = data['drug'].y\n",
    "test_data['drug'].x = data['drug'].x\n",
    "test_data['drug'].y = data['drug'].y\n",
    "\n",
    "# Create train, validation, and test masks for nodes\n",
    "def create_node_masks(data, num_nodes, train_ratio=0.8, val_ratio=0.1):\n",
    "    train_mask, test_mask = train_test_split(range(num_nodes), test_size=1-train_ratio, random_state=42)\n",
    "    train_mask, val_mask = train_test_split(train_mask, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    mask_dict = {\n",
    "        'train_mask': torch.zeros(num_nodes, dtype=torch.bool),\n",
    "        'val_mask': torch.zeros(num_nodes, dtype=torch.bool),\n",
    "        'test_mask': torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    }\n",
    "    \n",
    "    mask_dict['train_mask'][train_mask] = True\n",
    "    mask_dict['val_mask'][val_mask] = True\n",
    "    mask_dict['test_mask'][test_mask] = True\n",
    "    \n",
    "    return mask_dict\n",
    "\n",
    "node_masks = create_node_masks(data, num_nodes)\n",
    "\n",
    "train_data['drug'].train_mask = node_masks['train_mask']\n",
    "train_data['drug'].val_mask = node_masks['val_mask']\n",
    "train_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "val_data['drug'].train_mask = node_masks['train_mask']\n",
    "val_data['drug'].val_mask = node_masks['val_mask']\n",
    "val_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "test_data['drug'].train_mask = node_masks['train_mask']\n",
    "test_data['drug'].val_mask = node_masks['val_mask']\n",
    "test_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "# Print data to verify the splits\n",
    "print(\"Train Data:\", train_data)\n",
    "print(\"Validation Data:\", val_data)\n",
    "print(\"Test Data:\", test_data)\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'targets', 'target'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'isDoping', 'doping'): GraphConv(-1, hidden_channels),\n",
    "            ('drug', 'interactsWith', 'drug'): GraphConv(-1, hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "\n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "\n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_data)\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(train_data, train_data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(val_data, val_data['drug'].val_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(test_data, test_data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 6009] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 1604] },\n",
      "  (drug, targets, target)={ edge_index=[2, 243] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 243] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 46111] }\n",
      ")\n",
      "Validation Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 668] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 179] },\n",
      "  (drug, targets, target)={ edge_index=[2, 27] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 27] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 5124] }\n",
      ")\n",
      "Test Data: HeteroData(\n",
      "  drug={\n",
      "    x=[338, 467],\n",
      "    y=[338],\n",
      "    train_mask=[338],\n",
      "    val_mask=[338],\n",
      "    test_mask=[338],\n",
      "  },\n",
      "  drug_category={ x=[1095, 1095] },\n",
      "  atc_code={ x=[974, 974] },\n",
      "  target={ x=[249, 249] },\n",
      "  doping={ x=[2, 2] },\n",
      "  (drug, isInCategory, drug_category)={ edge_index=[2, 1670] },\n",
      "  (drug, isClassifiedAs, atc_code)={ edge_index=[2, 446] },\n",
      "  (drug, targets, target)={ edge_index=[2, 68] },\n",
      "  (drug, isDoping, doping)={ edge_index=[2, 68] },\n",
      "  (drug, interactsWith, drug)={ edge_index=[2, 12809] }\n",
      ")\n",
      "Epoch: 001, Loss: 2.9738, Train Acc: 0.6132\n",
      "Epoch: 002, Loss: 6.4971, Train Acc: 0.6132\n",
      "Epoch: 003, Loss: 4.8746, Train Acc: 0.6337\n",
      "Epoch: 004, Loss: 1.6063, Train Acc: 0.3868\n",
      "Epoch: 005, Loss: 3.0758, Train Acc: 0.3868\n",
      "Epoch: 006, Loss: 2.6831, Train Acc: 0.4156\n",
      "Epoch: 007, Loss: 1.1303, Train Acc: 0.6337\n",
      "Epoch: 008, Loss: 0.7891, Train Acc: 0.6337\n",
      "Epoch: 009, Loss: 1.2431, Train Acc: 0.6337\n",
      "Epoch: 010, Loss: 1.3761, Train Acc: 0.6337\n",
      "Epoch: 011, Loss: 1.2319, Train Acc: 0.6337\n",
      "Epoch: 012, Loss: 0.8690, Train Acc: 0.6543\n",
      "Epoch: 013, Loss: 0.6451, Train Acc: 0.4609\n",
      "Epoch: 014, Loss: 0.8656, Train Acc: 0.4527\n",
      "Epoch: 015, Loss: 0.8870, Train Acc: 0.4815\n",
      "Epoch: 016, Loss: 0.7495, Train Acc: 0.5967\n",
      "Epoch: 017, Loss: 0.6454, Train Acc: 0.6337\n",
      "Epoch: 018, Loss: 0.6268, Train Acc: 0.6379\n",
      "Epoch: 019, Loss: 0.6326, Train Acc: 0.6379\n",
      "Epoch: 020, Loss: 0.6372, Train Acc: 0.6337\n",
      "Epoch: 021, Loss: 0.6391, Train Acc: 0.6337\n",
      "Epoch: 022, Loss: 0.6378, Train Acc: 0.6337\n",
      "Epoch: 023, Loss: 0.6348, Train Acc: 0.6379\n",
      "Epoch: 024, Loss: 0.6316, Train Acc: 0.6379\n",
      "Epoch: 025, Loss: 0.6290, Train Acc: 0.6379\n",
      "Epoch: 026, Loss: 0.6275, Train Acc: 0.6379\n",
      "Epoch: 027, Loss: 0.6271, Train Acc: 0.6379\n",
      "Epoch: 028, Loss: 0.6271, Train Acc: 0.6420\n",
      "Epoch: 029, Loss: 0.6264, Train Acc: 0.6173\n",
      "Epoch: 030, Loss: 0.6381, Train Acc: 0.6337\n",
      "Epoch: 031, Loss: 0.6293, Train Acc: 0.6337\n",
      "Epoch: 032, Loss: 0.6272, Train Acc: 0.6337\n",
      "Epoch: 033, Loss: 0.6273, Train Acc: 0.6379\n",
      "Epoch: 034, Loss: 0.6265, Train Acc: 0.6379\n",
      "Epoch: 035, Loss: 0.6253, Train Acc: 0.6420\n",
      "Epoch: 036, Loss: 0.6241, Train Acc: 0.6420\n",
      "Epoch: 037, Loss: 0.6230, Train Acc: 0.6379\n",
      "Epoch: 038, Loss: 0.6222, Train Acc: 0.6379\n",
      "Epoch: 039, Loss: 0.6217, Train Acc: 0.6379\n",
      "Epoch: 040, Loss: 0.6214, Train Acc: 0.6379\n",
      "Epoch: 041, Loss: 0.6208, Train Acc: 0.6379\n",
      "Epoch: 042, Loss: 0.6197, Train Acc: 0.6379\n",
      "Epoch: 043, Loss: 0.6185, Train Acc: 0.6379\n",
      "Epoch: 044, Loss: 0.6172, Train Acc: 0.6420\n",
      "Epoch: 045, Loss: 0.6159, Train Acc: 0.6420\n",
      "Epoch: 046, Loss: 0.6146, Train Acc: 0.6420\n",
      "Epoch: 047, Loss: 0.6132, Train Acc: 0.6379\n",
      "Epoch: 048, Loss: 0.6120, Train Acc: 0.6420\n",
      "Epoch: 049, Loss: 0.6108, Train Acc: 0.6461\n",
      "Epoch: 050, Loss: 0.6097, Train Acc: 0.6543\n",
      "Epoch: 051, Loss: 0.6087, Train Acc: 0.6626\n",
      "Epoch: 052, Loss: 0.6076, Train Acc: 0.6708\n",
      "Epoch: 053, Loss: 0.6064, Train Acc: 0.6749\n",
      "Epoch: 054, Loss: 0.6052, Train Acc: 0.6708\n",
      "Epoch: 055, Loss: 0.6039, Train Acc: 0.6749\n",
      "Epoch: 056, Loss: 0.6025, Train Acc: 0.6708\n",
      "Epoch: 057, Loss: 0.6012, Train Acc: 0.6626\n",
      "Epoch: 058, Loss: 0.5999, Train Acc: 0.6626\n",
      "Epoch: 059, Loss: 0.5987, Train Acc: 0.6626\n",
      "Epoch: 060, Loss: 0.5975, Train Acc: 0.6708\n",
      "Epoch: 061, Loss: 0.5962, Train Acc: 0.6749\n",
      "Epoch: 062, Loss: 0.5950, Train Acc: 0.6790\n",
      "Epoch: 063, Loss: 0.5937, Train Acc: 0.6831\n",
      "Epoch: 064, Loss: 0.5924, Train Acc: 0.6914\n",
      "Epoch: 065, Loss: 0.5912, Train Acc: 0.6914\n",
      "Epoch: 066, Loss: 0.5901, Train Acc: 0.6955\n",
      "Epoch: 067, Loss: 0.5890, Train Acc: 0.7037\n",
      "Epoch: 068, Loss: 0.5879, Train Acc: 0.7037\n",
      "Epoch: 069, Loss: 0.5868, Train Acc: 0.7037\n",
      "Epoch: 070, Loss: 0.5856, Train Acc: 0.6955\n",
      "Epoch: 071, Loss: 0.5844, Train Acc: 0.6955\n",
      "Epoch: 072, Loss: 0.5833, Train Acc: 0.6996\n",
      "Epoch: 073, Loss: 0.5821, Train Acc: 0.6996\n",
      "Epoch: 074, Loss: 0.5810, Train Acc: 0.6955\n",
      "Epoch: 075, Loss: 0.5798, Train Acc: 0.7037\n",
      "Epoch: 076, Loss: 0.5786, Train Acc: 0.7202\n",
      "Epoch: 077, Loss: 0.5774, Train Acc: 0.7160\n",
      "Epoch: 078, Loss: 0.5763, Train Acc: 0.7119\n",
      "Epoch: 079, Loss: 0.5751, Train Acc: 0.7119\n",
      "Epoch: 080, Loss: 0.5739, Train Acc: 0.7160\n",
      "Epoch: 081, Loss: 0.5727, Train Acc: 0.7160\n",
      "Epoch: 082, Loss: 0.5716, Train Acc: 0.7202\n",
      "Epoch: 083, Loss: 0.5704, Train Acc: 0.7202\n",
      "Epoch: 084, Loss: 0.5692, Train Acc: 0.7160\n",
      "Epoch: 085, Loss: 0.5680, Train Acc: 0.7160\n",
      "Epoch: 086, Loss: 0.5668, Train Acc: 0.7078\n",
      "Epoch: 087, Loss: 0.5656, Train Acc: 0.7037\n",
      "Epoch: 088, Loss: 0.5645, Train Acc: 0.7037\n",
      "Epoch: 089, Loss: 0.5633, Train Acc: 0.7037\n",
      "Epoch: 090, Loss: 0.5621, Train Acc: 0.7078\n",
      "Epoch: 091, Loss: 0.5609, Train Acc: 0.7078\n",
      "Epoch: 092, Loss: 0.5597, Train Acc: 0.7078\n",
      "Epoch: 093, Loss: 0.5585, Train Acc: 0.7078\n",
      "Epoch: 094, Loss: 0.5572, Train Acc: 0.7119\n",
      "Epoch: 095, Loss: 0.5560, Train Acc: 0.7119\n",
      "Epoch: 096, Loss: 0.5548, Train Acc: 0.7119\n",
      "Epoch: 097, Loss: 0.5535, Train Acc: 0.7119\n",
      "Epoch: 098, Loss: 0.5523, Train Acc: 0.7119\n",
      "Epoch: 099, Loss: 0.5510, Train Acc: 0.7160\n",
      "Epoch: 100, Loss: 0.5497, Train Acc: 0.7160\n",
      "Average Train Precision: 0.7359, Average Train Recall: 0.5777, Average Train F1: 0.5247\n",
      "Average Val Precision: 0.6507, Average Val Recall: 0.6201, Average Val F1: 0.5911\n",
      "Test Acc: 0.5882, Test Precision: 0.4976, Test Recall: 0.4986, Test F1: 0.4711\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume data is your input HeteroData\n",
    "data = ToUndirected()(data_small)\n",
    "\n",
    "# Create labels for the 'drug' nodes based on their connections to 'doping'\n",
    "doping_labels = torch.zeros(data['drug'].x.size(0), dtype=torch.long)\n",
    "\n",
    "# Assuming encoded_doping_df has the doping information\n",
    "# 0 means not doping, 1 means is doping\n",
    "for drug_idx, doping in enumerate(encoded_doping_df['Doping']):\n",
    "    if doping == 1:\n",
    "        doping_labels[drug_idx] = 1\n",
    "\n",
    "data['drug'].y = doping_labels\n",
    "\n",
    "def split_edges(data, edge_type, test_size=0.2, val_size=0.1):\n",
    "    edge_index = data[edge_type].edge_index.numpy()\n",
    "    num_edges = edge_index.shape[1]\n",
    "    \n",
    "    # Split the edges into training and testing sets\n",
    "    train_edges, test_edges = train_test_split(range(num_edges), test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Further split the training edges into training and validation sets\n",
    "    train_edges, val_edges = train_test_split(train_edges, test_size=val_size, random_state=42)\n",
    "    \n",
    "    # Create edge indices\n",
    "    train_edge_index = torch.tensor(edge_index[:, train_edges], dtype=torch.long)\n",
    "    val_edge_index = torch.tensor(edge_index[:, val_edges], dtype=torch.long)\n",
    "    test_edge_index = torch.tensor(edge_index[:, test_edges], dtype=torch.long)\n",
    "    \n",
    "    return train_edge_index, val_edge_index, test_edge_index\n",
    "\n",
    "# Define edge types\n",
    "edge_types = [\n",
    "    ('drug', 'isInCategory', 'drug_category'),\n",
    "    ('drug', 'isClassifiedAs', 'atc_code'),\n",
    "    ('drug', 'targets', 'target'),\n",
    "    ('drug', 'isDoping', 'doping'),\n",
    "    ('drug', 'interactsWith', 'drug')\n",
    "]\n",
    "\n",
    "# Initialize the train, validation, and test data\n",
    "train_data = HeteroData()\n",
    "val_data = HeteroData()\n",
    "test_data = HeteroData()\n",
    "\n",
    "for edge_type in edge_types:\n",
    "    train_edge_index, val_edge_index, test_edge_index = split_edges(data, edge_type)\n",
    "    \n",
    "    train_data[edge_type].edge_index = train_edge_index\n",
    "    val_data[edge_type].edge_index = val_edge_index\n",
    "    test_data[edge_type].edge_index = test_edge_index\n",
    "    \n",
    "    if 'x' in data[edge_type[0]]:\n",
    "        train_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        val_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        test_data[edge_type[0]].x = data[edge_type[0]].x\n",
    "        \n",
    "    if 'x' in data[edge_type[2]]:\n",
    "        train_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "        val_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "        test_data[edge_type[2]].x = data[edge_type[2]].x\n",
    "\n",
    "# Set the node features and labels for the 'drug' nodes in the train, validation, and test data\n",
    "num_nodes = data['drug'].x.size(0)\n",
    "train_data['drug'].x = data['drug'].x\n",
    "train_data['drug'].y = data['drug'].y\n",
    "val_data['drug'].x = data['drug'].x\n",
    "val_data['drug'].y = data['drug'].y\n",
    "test_data['drug'].x = data['drug'].x\n",
    "test_data['drug'].y = data['drug'].y\n",
    "\n",
    "# Create train, validation, and test masks for nodes\n",
    "def create_node_masks(data, num_nodes, train_ratio=0.8, val_ratio=0.1):\n",
    "    train_mask, test_mask = train_test_split(range(num_nodes), test_size=1-train_ratio, random_state=42)\n",
    "    train_mask, val_mask = train_test_split(train_mask, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    mask_dict = {\n",
    "        'train_mask': torch.zeros(num_nodes, dtype=torch.bool),\n",
    "        'val_mask': torch.zeros(num_nodes, dtype=torch.bool),\n",
    "        'test_mask': torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    }\n",
    "    \n",
    "    mask_dict['train_mask'][train_mask] = True\n",
    "    mask_dict['val_mask'][val_mask] = True\n",
    "    mask_dict['test_mask'][test_mask] = True\n",
    "    \n",
    "    return mask_dict\n",
    "\n",
    "node_masks = create_node_masks(data, num_nodes)\n",
    "\n",
    "train_data['drug'].train_mask = node_masks['train_mask']\n",
    "train_data['drug'].val_mask = node_masks['val_mask']\n",
    "train_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "val_data['drug'].train_mask = node_masks['train_mask']\n",
    "val_data['drug'].val_mask = node_masks['val_mask']\n",
    "val_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "test_data['drug'].train_mask = node_masks['train_mask']\n",
    "test_data['drug'].val_mask = node_masks['val_mask']\n",
    "test_data['drug'].test_mask = node_masks['test_mask']\n",
    "\n",
    "# Print data to verify the splits\n",
    "print(\"Train Data:\", train_data)\n",
    "print(\"Validation Data:\", val_data)\n",
    "print(\"Test Data:\", test_data)\n",
    "\n",
    "class HeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super(HeteroGNN, self).__init__()\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('drug', 'isInCategory', 'drug_category'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'isClassifiedAs', 'atc_code'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'targets', 'target'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'isDoping', 'doping'): SAGEConv(-1, hidden_channels),\n",
    "            ('drug', 'interactsWith', 'drug'): SAGEConv(-1, hidden_channels),\n",
    "        }, aggr='sum')\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        return self.lin(x_dict['drug'])\n",
    "\n",
    "# Define the model, optimizer, and loss function\n",
    "model = HeteroGNN(hidden_channels=64, out_channels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "\n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "\n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n",
    "\n",
    "# Training loop\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_data)\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(train_data, train_data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(val_data, val_data['drug'].val_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(test_data, test_data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "c:\\Users\\agavr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 38>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m train_acc, train_precision, train_recall, train_f1 \u001b[38;5;241m=\u001b[39m test(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtrain_mask)\n\u001b[0;32m     41\u001b[0m val_acc, val_precision, val_recall, val_f1 \u001b[38;5;241m=\u001b[39m test(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mval_mask)\n\u001b[1;32m---> 42\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdrug\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m train_precisions\u001b[38;5;241m.\u001b[39mappend(train_precision)\n\u001b[0;32m     45\u001b[0m train_recalls\u001b[38;5;241m.\u001b[39mappend(train_recall)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(mask)\u001b[0m\n\u001b[0;32m     24\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39my[mask]\u001b[38;5;241m.\u001b[39mcpu(), pred[mask]\u001b[38;5;241m.\u001b[39mcpu(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrug\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39my[mask]\u001b[38;5;241m.\u001b[39mcpu(), pred[mask]\u001b[38;5;241m.\u001b[39mcpu(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorrect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, precision, recall, f1\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    loss = criterion(out[data['drug'].train_mask], data['drug'].y[data['drug'].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x_dict, data.edge_index_dict)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = pred[mask] == data['drug'].y[mask]\n",
    "        \n",
    "        precision = precision_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        recall = recall_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        f1 = f1_score(data['drug'].y[mask].cpu(), pred[mask].cpu(), average='macro', zero_division=0)\n",
    "        \n",
    "        return int(correct.sum()) / int(mask.sum()), precision, recall, f1\n",
    "\n",
    "# To accumulate metrics across epochs\n",
    "train_precisions = []\n",
    "train_recalls = []\n",
    "train_f1s = []\n",
    "\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1s = []\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, train_precision, train_recall, train_f1 = test(data['drug'].train_mask)\n",
    "    val_acc, val_precision, val_recall, val_f1 = test(data['drug'].val_mask)\n",
    "    test_acc = test(data['drug'].test_mask)\n",
    "    \n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "    train_f1s.append(train_f1)\n",
    "    \n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1s.append(val_f1)\n",
    "    \n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "# Calculate the average metrics over all epochs\n",
    "avg_train_precision = sum(train_precisions) / len(train_precisions)\n",
    "avg_train_recall = sum(train_recalls) / len(train_recalls)\n",
    "avg_train_f1 = sum(train_f1s) / len(train_f1s)\n",
    "\n",
    "avg_val_precision = sum(val_precisions) / len(val_precisions)\n",
    "avg_val_recall = sum(val_recalls) / len(val_recalls)\n",
    "avg_val_f1 = sum(val_f1s) / len(val_f1s)\n",
    "\n",
    "print(f'Average Train Precision: {avg_train_precision:.4f}, Average Train Recall: {avg_train_recall:.4f}, Average Train F1: {avg_train_f1:.4f}')\n",
    "print(f'Average Val Precision: {avg_val_precision:.4f}, Average Val Recall: {avg_val_recall:.4f}, Average Val F1: {avg_val_f1:.4f}')\n",
    "\n",
    "# Test the model\n",
    "test_acc, test_precision, test_recall, test_f1 = test(data['drug'].test_mask)\n",
    "print(f'Test Acc: {test_acc:.4f}, Test Precision: {test_precision:.4f}, Test Recall: {test_recall:.4f}, Test F1: {test_f1:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
